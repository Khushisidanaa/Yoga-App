{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from opencv-python) (1.21.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 06:04:18.814523: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zh/lw_vkl357q15gsm73_v0m0lh0000gn/T/ipykernel_4512/3644367403.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.0.2-cp37-cp37m-macosx_10_13_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn) (1.21.6)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting scipy>=1.1.0\n",
      "  Using cached scipy-1.7.3-cp37-cp37m-macosx_10_9_x86_64.whl (33.0 MB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.0.2 scipy-1.7.3 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip3 install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'examples'...\n",
      "remote: Enumerating objects: 23066, done.\u001b[K\n",
      "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
      "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
      "remote: Total 23066 (delta 6), reused 7 (delta 0), pack-reused 23038\u001b[K\n",
      "Receiving objects: 100% (23066/23066), 42.17 MiB | 4.23 MiB/s, done.\n",
      "Resolving deltas: 100% (12655/12655), done.\n",
      "Updating files: 100% (2762/2762), done.\n"
     ]
    }
   ],
   "source": [
    "# Download model from TF Hub and check out inference code from GitHub\n",
    "\n",
    "!git clone https://github.com/tensorflow/examples.git\n",
    "pose_sample_rpi_path = os.path.join(os.getcwd(), 'examples/lite/examples/pose_estimation/raspberry_pi')\n",
    "sys.path.append(pose_sample_rpi_path)\n",
    "\n",
    "# Load MoveNet Thunder model\n",
    "import utils\n",
    "from data import BodyPart\n",
    "from ml import Movenet\n",
    "movenet = Movenet('movenet_thunder')\n",
    "\n",
    "# Define function to run pose estimation using MoveNet Thunder.\n",
    "# You'll apply MoveNet's cropping algorithm and run inference multiple times on\n",
    "# the input image to improve pose estimation accuracy.\n",
    "def detect(input_tensor, inference_count=3):\n",
    "  \"\"\"Runs detection on an input image.\n",
    "\n",
    "  Args:\n",
    "    input_tensor: A [height, width, 3] Tensor of type tf.float32.\n",
    "      Note that height and width can be anything since the image will be\n",
    "      immediately resized according to the needs of the model within this\n",
    "      function.\n",
    "    inference_count: Number of times the model should run repeatly on the\n",
    "      same input image to improve detection accuracy.\n",
    "\n",
    "  Returns:\n",
    "    A Person entity detected by the MoveNet.SinglePose.\n",
    "  \"\"\"\n",
    "  image_height, image_width, channel = input_tensor.shape\n",
    "\n",
    "  # Detect pose using the full input image\n",
    "  movenet.detect(input_tensor.numpy(), reset_crop_region=True)\n",
    "\n",
    "  # Repeatedly using previous detection result to identify the region of\n",
    "  # interest and only croping that region to improve detection accuracy\n",
    "  for _ in range(inference_count - 1):\n",
    "    person = movenet.detect(input_tensor.numpy(), \n",
    "                            reset_crop_region=False)\n",
    "\n",
    "  return person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_prediction_on_image(\n",
    "    image, person, crop_region=None, close_figure=True,\n",
    "    keep_input_size=False):\n",
    "  \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "  Args:\n",
    "    image: An numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    person: A person entity returned from the MoveNet.SinglePose model.\n",
    "    close_figure: Whether to close the plt figure after the function returns.\n",
    "    keep_input_size: Whether to keep the size of the input image.\n",
    "\n",
    "  Returns:\n",
    "    An numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "  \"\"\"\n",
    "  # Draw the detection result on top of the image.\n",
    "  image_np = utils.visualize(image, [person])\n",
    "\n",
    "  # Plot the image with detection results.\n",
    "  height, width, channel = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "  im = ax.imshow(image_np)\n",
    "\n",
    "  if close_figure:\n",
    "    plt.close(fig)\n",
    "\n",
    "  if not keep_input_size:\n",
    "    image_np = utils.keep_aspect_ratio_resizer(image_np, (512, 512))\n",
    "\n",
    "  return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Functions to visualize the pose estimation results.\n",
    "\n",
    "def draw_prediction_on_image(\n",
    "    image, person, crop_region=None, close_figure=True,\n",
    "    keep_input_size=False):\n",
    "  \"\"\"Draws the keypoint predictions on image.\n",
    " \n",
    "  Args:\n",
    "    image: An numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    person: A person entity returned from the MoveNet.SinglePose model.\n",
    "    close_figure: Whether to close the plt figure after the function returns.\n",
    "    keep_input_size: Whether to keep the size of the input image.\n",
    " \n",
    "  Returns:\n",
    "    An numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "  \"\"\"\n",
    "  # Draw the detection result on top of the image.\n",
    "  image_np = utils.visualize(image, [person])\n",
    "  \n",
    "  # Plot the image with detection results.\n",
    "  height, width, channel = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "  im = ax.imshow(image_np)\n",
    " \n",
    "  if close_figure:\n",
    "    plt.close(fig)\n",
    " \n",
    "  if not keep_input_size:\n",
    "    image_np = utils.keep_aspect_ratio_resizer(image_np, (512, 512))\n",
    "\n",
    "  return image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "is_skip_step_1 = False #@param [\"False\", \"True\"] {type:\"raw\"}\n",
    "\n",
    "\n",
    "class MoveNetPreprocessor1(object):\n",
    "    \"\"\"Helper class to preprocess pose sample images for classification.\"\"\"\n",
    "\n",
    "    def __init__(self, images_out_folder, csv_out_path):\n",
    "        \"\"\"Creates a preprocessor to detect pose from a single image and save the result as a CSV.\n",
    "\n",
    "        Args:\n",
    "            images_out_folder: Path to write the image overlay with detected landmarks. This image is useful when you need to debug accuracy issues.\n",
    "            csv_out_path: Path to write the CSV containing the detected landmark coordinates and label of the image that can be used to train a pose classification model.\n",
    "        \"\"\"\n",
    "        self._images_out_folder = images_out_folder\n",
    "        self._csv_out_path = csv_out_path\n",
    "        self._messages = []\n",
    "\n",
    "    def process(self, image_path, detection_threshold=0.1):\n",
    "        \"\"\"Preprocesses a single image.\n",
    "\n",
    "        Args:\n",
    "            image_path: Path to the input image.\n",
    "            detection_threshold: Only keep the image if all landmark confidence scores are above this threshold.\n",
    "        \"\"\"\n",
    "        # Detect landmarks in the image and write it to a CSV file\n",
    "        with open(self._csv_out_path, 'w') as csv_out_file:\n",
    "            csv_out_writer = csv.writer(csv_out_file, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "            try:\n",
    "                image = tf.io.read_file(image_path)\n",
    "                image = tf.io.decode_jpeg(image)\n",
    "            except:\n",
    "                self._messages.append('Skipped ' + image_path + '. Invalid image.')\n",
    "            else:\n",
    "                image_height, image_width, channel = image.shape\n",
    "\n",
    "                # Skip images that aren't RGB because Movenet requires RGB images\n",
    "                if channel != 3:\n",
    "                    self._messages.append('Skipped ' + image_path + '. Image isn\\'t in RGB format.')\n",
    "                else:\n",
    "                    person = detect(image)\n",
    "\n",
    "                    # Save landmarks if all landmarks were detected\n",
    "                    min_landmark_score = min([keypoint.score for keypoint in person.keypoints])\n",
    "                    should_keep_image = min_landmark_score >= detection_threshold\n",
    "                    if not should_keep_image:\n",
    "                        self._messages.append('Skipped ' + image_path + '. No pose was confidently detected.')\n",
    "                    else:\n",
    "                        # Draw the prediction result on top of the image for debugging later\n",
    "                        output_overlay = draw_prediction_on_image(\n",
    "                            image.numpy().astype(np.uint8), person, close_figure=True, keep_input_size=True)\n",
    "\n",
    "                        # Write detection result into an image file\n",
    "                        output_frame = cv2.cvtColor(output_overlay, cv2.COLOR_RGB2BGR)\n",
    "                        filename = os.path.basename(image_path)\n",
    "                        cv2.imwrite(os.path.join(self._images_out_folder, filename), output_frame)\n",
    "\n",
    "                        # Get landmarks and scale them to the same size as the input image\n",
    "                        pose_landmarks = np.array(\n",
    "                            [[keypoint.coordinate.x, keypoint.coordinate.y, keypoint.score] for keypoint in person.keypoints],\n",
    "                            dtype=np.float32)\n",
    "\n",
    "                        # Write the landmark coordinates to the CSV file\n",
    "                        coordinates = pose_landmarks.flatten().astype(str).tolist()\n",
    "                        csv_out_writer.writerow([os.path.basename(image_path)] + coordinates)\n",
    "\n",
    "        if not os.path.exists(self._images_out_folder):\n",
    "            os.makedirs(self._images_out_folder)\n",
    "\n",
    "        # Print the error message collected\n",
    "        for message in self._messages:\n",
    "            print(message, file=sys.stderr)\n",
    "\n",
    "if not is_skip_step_1:\n",
    "  images_out_train_folder = 'abc'\n",
    "  csvs_out_train_path = 'train_data1.csv'\n",
    "\n",
    "  preprocessor = MoveNetPreprocessor1(\n",
    "      \n",
    "      images_out_folder=images_out_train_folder,\n",
    "      csv_out_path=csvs_out_train_path,\n",
    "  )\n",
    "\n",
    "  preprocessor.process('yoga_poses/train/tree/girl1_tree096.jpg')\n",
    "      \n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
